{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator Benchmarks\n",
    "\n",
    "Notebook for benchmarking known IV estimators against different data generating processes.\n",
    "\n",
    "Current roster:\n",
    "- Split-sample IV\n",
    "- 2SLS\n",
    "- Jackknife IV\n",
    "- LIML **TODO**\n",
    "- Mostly harmless ML **TODO**\n",
    "- DeepIV **TODO**\n",
    "- DoubleML **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from linearmodels.iv import IV2SLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is assuming a notebook placed inside the notebooks/ folder\n",
    "import sys\n",
    "sys.path.append('../src/data')\n",
    "from lin_norm_generator import LinearNormalDataGenerator\n",
    "\n",
    "df = LinearNormalDataGenerator().generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabPFN-style data generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VARS = 100\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    Lifted from https://github.com/automl/TabPFN/blob/main/tabpfn/priors/mlp.py\n",
    "    TODO introduce shared covariance matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, std, device):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + torch.normal(torch.zeros_like(x), self.std)\n",
    "\n",
    "\n",
    "def causes_sampler_f(num_causes):\n",
    "    means = np.random.normal(0, 1, (num_causes))\n",
    "    std = np.abs(np.random.normal(0, 1, (num_causes)) * means)\n",
    "    return means, std\n",
    "\n",
    "class IVGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to generate IV datasets.\n",
    "\n",
    "    TODO add ability to generate IV controls to cover the Angrist/Frandsen case\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                tau: float,\n",
    "                max_vars: int,\n",
    "                n_instruments: int,\n",
    "                instrument_covariance: torch.Tensor,\n",
    "                instrument_strength: float, # this currently translates to mu^2/n_samples\n",
    "                tau_activation: str = 'identity',\n",
    "                instrument_activation: str = 'identity',\n",
    "                # TODO add controls as well\n",
    "                control_activation: str = 'identity',\n",
    "                control_covariance: torch.Tensor = None, \n",
    "                control_str: float = 0,\n",
    "                n_controls: int = 0,\n",
    "\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO are these actually needed?\n",
    "        #self.controls = nn.Linear(max_vars, 1)\n",
    "        self.instruments = nn.Linear(max_vars, 1)\n",
    "\n",
    "        #Nself.n_controls = n_controls\n",
    "        self.n_instruments = n_instruments\n",
    "\n",
    "        if control_covariance is not None:\n",
    "            assert control_covariance.shape == (n_controls, n_controls)\n",
    "\n",
    "        assert instrument_covariance.shape == (n_instruments, n_instruments)\n",
    "        self.instrument_covariance = instrument_covariance\n",
    "        self.instrument_sampler = MultivariateNormal(torch.zeros(self.n_instruments), self.instrument_covariance)\n",
    "        \n",
    "        # currently follows the Beta pattern of Lennon et al. 2022, corresponds to their pi\n",
    "        self.instrument_coefficients = torch.pow(torch.ones(self.n_instruments) * 0.7, \n",
    "                                         torch.arange(0, self.n_instruments))\n",
    "\n",
    "        # shuffle coefficients\n",
    "        self.instrument_coefficients = self.instrument_coefficients[torch.randperm(self.n_instruments)]\n",
    "\n",
    "        self.sigma_v = (torch.t(self.instrument_coefficients) @ self.instrument_covariance @ self.instrument_coefficients) / instrument_strength\n",
    "        self.sigma_v = torch.sqrt(self.sigma_v)\n",
    "        self.sigma_y = 1\n",
    "        \n",
    "        # TODO need to ensure that confounder covariance is positive definite\n",
    "        self.confound_covariance = torch.Tensor([[self.sigma_y**2, self.sigma_y*self.sigma_v], \n",
    "                                                 [self.sigma_y*self.sigma_v, self.sigma_v**2]])\n",
    "        self.confound_sampler = MultivariateNormal(torch.zeros(2), self.confound_covariance)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.control_str = control_str\n",
    "        \n",
    "        self.activations = {\n",
    "            'identity': lambda x: x,\n",
    "            'relu': F.relu,\n",
    "            'sigmoid': F.sigmoid,\n",
    "            'tanh': F.tanh,\n",
    "            'softplus': F.softplus,\n",
    "            'leaky_relu': F.leaky_relu,\n",
    "            'elu': F.elu,\n",
    "        }\n",
    "    \n",
    "        self.tau_activation = self.activations[tau_activation]\n",
    "        self.instrument_activation = self.activations[instrument_activation]\n",
    "        #self.confounder_activation = self.activations[confounder_activation]\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Generates a single data sample\"\"\"\n",
    "        #confound_sample = torch.cat([torch.randn(self.n_confounds), torch.zeros(MAX_VARS - self.n_confounds)])\n",
    "\n",
    "        # noise sample [\\epislon_y, \\epeilson_v] according to confounder covariance\n",
    "        #noise_sample = torch.normal(TODO)\n",
    "        instrument_sample = torch.cat([self.instrument_sampler.sample(), torch.zeros(MAX_VARS - self.n_instruments)])\n",
    "\n",
    "        epislon_y, epsilon_v = self.confound_sampler.sample()\n",
    "        #confounds = self.confounder_activation(self.confounders(confound_sample))\n",
    "        pi = torch.cat([self.instrument_coefficients, torch.zeros(MAX_VARS - self.n_instruments)])\n",
    "        treat = self.instrument_activation(torch.t(pi) @ instrument_sample) + epsilon_v\n",
    "        \n",
    "        #print(treat.shape)\n",
    "\n",
    "        outcome = self.tau*self.tau_activation(treat) + torch.randn(1) + epislon_y\n",
    "\n",
    "        # return data matrix of T, Y, Z\n",
    "        return torch.cat([torch.Tensor([treat, outcome]), instrument_sample])\n",
    "\n",
    "        # return data matrix of T, Y, X, Z\n",
    "        #return torch.cat([treat, outcome, confound_sample, instrument_sample])\n",
    "        \n",
    "    def batch(self, batch_size: int):\n",
    "        \"\"\"Generate batch of examples\"\"\"\n",
    "        return torch.stack([self.forward() for _ in range(batch_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "mu2 = 2000\n",
    "iv_gen = IVGenerator(\n",
    "            tau=1,\n",
    "            max_vars=MAX_VARS,\n",
    "            n_instruments=7,\n",
    "            instrument_covariance=torch.Tensor([[1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                                                [0.5, 1, 0.5, 0.5, 0.5, 0.5, 0.5],\n",
    "                                                [0.5, 0.5, 1, 0.5, 0.5, 0.5, 0.5],\n",
    "                                                [0.5, 0.5, 0.5, 1, 0.5, 0.5, 0.5],\n",
    "                                                [0.5, 0.5, 0.5, 0.5, 1, 0.5, 0.5],\n",
    "                                                [0.5, 0.5, 0.5, 0.5, 0.5, 1, 0.5],\n",
    "                                                [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1]]), \n",
    "            instrument_strength=mu2/n_samples,\n",
    "        )\n",
    "\n",
    "data = iv_gen.batch(n_samples).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 102)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lennon et al. 2022 Figure 2 Replication\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
