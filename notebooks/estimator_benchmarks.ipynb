{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator Benchmarks\n",
    "\n",
    "Notebook for benchmarking known IV estimators against different data generating processes.\n",
    "\n",
    "Current roster:\n",
    "- Split-sample IV\n",
    "- 2SLS\n",
    "- Jackknife IV\n",
    "- LIML **TODO**\n",
    "- Mostly harmless ML **TODO**\n",
    "- DeepIV **TODO**\n",
    "- DoubleML **TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from linearmodels.iv import IV2SLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is assuming a notebook placed inside the notebooks/ folder\n",
    "import sys\n",
    "sys.path.append('../src/data')\n",
    "from iv_data_generator import LinearNormalDataGenerator\n",
    "\n",
    "df = LinearNormalDataGenerator().generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabPFN-style data generating process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VARS = 100\n",
    "\n",
    "class GaussianNoise(nn.Module):\n",
    "    \"\"\"\n",
    "    Lifted from https://github.com/automl/TabPFN/blob/main/tabpfn/priors/mlp.py\n",
    "    TODO introduce shared covariance matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, std, device):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.device=device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + torch.normal(torch.zeros_like(x), self.std)\n",
    "\n",
    "\n",
    "def causes_sampler_f(num_causes):\n",
    "    means = np.random.normal(0, 1, (num_causes))\n",
    "    std = np.abs(np.random.normal(0, 1, (num_causes)) * means)\n",
    "    return means, std\n",
    "\n",
    "class IVGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to generate IV datasets.\n",
    "\n",
    "    TODO add ability to generate IV controls to cover the Angrist/Frandsen case\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                #n_controls: int,\n",
    "                n_instruments: int,\n",
    "                tau: float,\n",
    "                confound_str: float,\n",
    "                tau_activation: str = 'identity',\n",
    "                #control_activation: str = 'identity',\n",
    "                instrument_activation: str = 'identity',\n",
    "                confounder_covariance: torch.Tensor = None,\n",
    "                instrument_covariance: torch.Tensor = None,\n",
    "                max_vars: int = MAX_VARS\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.confounders = nn.Linear(max_vars, 1)\n",
    "        self.instruments = nn.Linear(max_vars, 1)\n",
    "\n",
    "        #self.n_confounds = n_confounds\n",
    "        self.n_instruments = n_instruments\n",
    "\n",
    "        assert confounder_covariance.shape == (n_confounds, n_confounds)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.confound_str = confound_str\n",
    "        \n",
    "        self.activations = {\n",
    "            'identity': lambda x: x,\n",
    "            'relu': F.relu,\n",
    "            'sigmoid': F.sigmoid,\n",
    "            'tanh': F.tanh,\n",
    "            'softplus': F.softplus,\n",
    "            'leaky_relu': F.leaky_relu,\n",
    "            'elu': F.elu,\n",
    "        }\n",
    "    \n",
    "        self.tau_activation = self.activations[tau_activation]\n",
    "        #self.confounder_activation = self.activations[confounder_activation]\n",
    "        self.instrument_activation = self.activations[instrument_activation]\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Generates a single data sample\"\"\"\n",
    "        #confound_sample = torch.cat([torch.randn(self.n_confounds), torch.zeros(MAX_VARS - self.n_confounds)])\n",
    "\n",
    "        # noise sample [\\epislon_y, \\epeilson_v] according to confounder covariance\n",
    "        noise_sample = torch.normal()\n",
    "        instrument_sample = torch.cat([torch.randn(self.n_instruments), torch.zeros(MAX_VARS - self.n_instruments)])\n",
    "\n",
    "        confounds = self.confounder_activation(self.confounders(confound_sample))\n",
    "\n",
    "        #print(confounds.shape)\n",
    "        #print(self.instrument_activation(self.instruments(instrument_sample)).shape)\n",
    "\n",
    "        treat = confounds + self.instrument_activation(self.instruments(instrument_sample))\n",
    "        \n",
    "        #print(treat.shape)\n",
    "\n",
    "        outcome = self.tau*self.tau_activation(treat) + confounds + torch.randn(1)\n",
    "\n",
    "        # return data matrix of T, Y, X, Z\n",
    "        return torch.cat([treat, outcome, confound_sample, instrument_sample])\n",
    "        \n",
    "    def batch(self, batch_size: int):\n",
    "        \"\"\"Generate batch of examples\"\"\"\n",
    "        return torch.stack([self.forward() for _ in range(batch_size)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv_gen = IVGenerator(n_confounds=1, \n",
    "                    n_instruments=7, \n",
    "                    tau=1,\n",
    "                    confound_str=1)\n",
    "\n",
    "data = iv_gen.batch(100).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 202)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.randn(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([tensor, torch.zeros(97)], dim=0).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lennon et al. 2022 Figure 2 Replication\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
