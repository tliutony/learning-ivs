{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Notebook for learning linear IVs with neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact_manual, IntSlider, FloatSlider\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from linearmodels.iv.model import IV2SLS\n",
    "from linearmodels.iv.model import _OLS\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_const_linear_iv(\n",
    "    n_samples,\n",
    "    seed,\n",
    "    pi,\n",
    "    psi,\n",
    "    tau,\n",
    "    gamma\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates linear IV with constant treatment effects.\n",
    "    \n",
    "    Args:\n",
    "        n_samples (int): num samples to generate\n",
    "        seed (int): seed for reproducibilty\n",
    "        pi (float): instrument \"strength\"\n",
    "        psi (float): confounding \"strength\"\n",
    "        tau (float): treatment effect\n",
    "        gamma (float): confound effect\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(seed),\n",
    "    Z = np.random.normal(0, 1, size=n_samples)#np.random.uniform(0, 10, n_samples)\n",
    "    C = np.random.normal(0, 1, size=n_samples)#np.random.uniform(0, 10, n_samples)\n",
    "    eta = np.random.normal(0, 1, size=n_samples)\n",
    "    const = np.random.uniform(-1, 1)\n",
    "\n",
    "    T = const + (pi*Z) + (psi*C) + eta\n",
    "\n",
    "    epsilon = np.random.normal(0, 1, size=n_samples)\n",
    "    beta = np.random.uniform(-1, 1)\n",
    "\n",
    "    Y = beta + (tau*T) + (gamma*C) + epsilon\n",
    "\n",
    "    data = np.concatenate([Z.reshape(-1,1), \n",
    "                           C.reshape(-1,1), \n",
    "                           T.reshape(-1,1),\n",
    "                           Y.reshape(-1,1),], \n",
    "                         axis=1)\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=['Z', 'C', 'T', 'Y'])\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:17<01:20,  8.93s/it]"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "\n",
    "}\n",
    "\n",
    "n_datasets = 10000\n",
    "n_test_datasets = 2000\n",
    "iv_strs = np.round(np.linspace(0, 2, 11), 2) \n",
    "\n",
    "for pi in tqdm(iv_strs):\n",
    "    datasets[pi] = {\n",
    "        \"data\": [],\n",
    "        \"taus\": np.zeros(n_datasets),\n",
    "        \"confounds\": np.zeros(n_datasets),\n",
    "        \"data_tup\": [],\n",
    "    }\n",
    "    # data = []#np.zeros((n_datasets, 10))\n",
    "    # taus = np.zeros(n_datasets)\n",
    "    # confounds = np.zeros(n_datasets)\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        seed = i + int(pi*10000) # to ensure we have non-overlapping datasets\n",
    "        np.random.seed(seed)\n",
    "        treat_effect = np.random.uniform(-2, 2)\n",
    "        confound_effect = 5 #np.random.uniform(1, 5) # hold confounding constant, for now TODO\n",
    "        psi_effect = np.random.uniform(5, 10)\n",
    "   \n",
    "        n_samples = 1000\n",
    "        \n",
    "        data_df = generate_const_linear_iv(\n",
    "            n_samples=n_samples,\n",
    "            seed=seed,\n",
    "            pi=pi,\n",
    "            psi=psi_effect,\n",
    "            tau=treat_effect,\n",
    "            gamma=confound_effect)\n",
    "        \n",
    "        # feats = generate_iv_features(data_df)\n",
    "        # data[i,:] = feats\n",
    "\n",
    "        # # zero out the variance of Y and variance of T\n",
    "        # data[i, feat_cols.index(\"var_Y\")] = 0\n",
    "        # data[i, feat_cols.index(\"var_T\")] = 0\n",
    "\n",
    "        datasets[pi]['data'].append(data_df)\n",
    "        datasets[pi]['taus'][i] = treat_effect\n",
    "        datasets[pi]['confounds'][i] = confound_effect\n",
    "\n",
    "        # convert data_df and tau to torch dataloader\n",
    "        datasets[pi]['data_tup'].append((data_df.drop(\"C\", axis='columns').values.astype('float32'), treat_effect))\n",
    "\n",
    "        # convert datasets data and taus to torch dataloader\n",
    "        train_data = torch.utils.data.DataLoader(\n",
    "            datasets[pi]['data_tup'][:n_datasets - n_test_datasets],\n",
    "            batch_size=32,\n",
    "        )\n",
    "\n",
    "        test_data = torch.utils.data.DataLoader(\n",
    "            datasets[pi]['data_tup'][n_test_datasets:],\n",
    "            batch_size=n_test_datasets,\n",
    "        )\n",
    "\n",
    "        datasets[pi]['train_data'] = train_data\n",
    "        datasets[pi]['test_data'] = train_data\n",
    "\n",
    "# for pi in datasets.keys():\n",
    "#     datasets[pi]['data'] = torch.utils.data.DataLoader(\n",
    "#         datasets[pi]['data'],\n",
    "#         batch_size=32,\n",
    "#         shuffle=True,\n",
    "#         num_workers=0,\n",
    "#     )\n",
    "#     datasets[pi]['taus'] = torch.from_numpy(datasets[pi]['taus']).float()\n",
    "#     datasets[pi]['confounds'] = torch.from_numpy(datasets[pi]['confounds']).float()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint autoencoder + treatment effect model\n",
    "class JointAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim):\n",
    "        super(JointAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "        self.treatment = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        z = self.encoder(x)\n",
    "        tau_hat = self.treatment(z)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z, tau_hat\n",
    "    \n",
    "\n",
    "# train the joint autoencoder\n",
    "def train_joint_autoencoder(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    lr,\n",
    "    device,\n",
    "    verbose=False,\n",
    "    batch_size=32,\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    test_x, test_y = next(iter(test_data))\n",
    "    test_x = test_x.to(torch.float32).to(device)\n",
    "    test_y = test_y.to(torch.float32).to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (data, labels) in enumerate(train_data):\n",
    "            data = data.to(torch.float32).to(device)\n",
    "            labels = labels.to(torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, z, tau_hat = model(data)\n",
    "            x_hat = x_hat.view(batch_size, -1, 3)\n",
    "            loss = criterion(x_hat, data) + criterion(tau_hat.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, _, tau_hat = model(test_x)\n",
    "            tau_hat = tau_hat.squeeze()\n",
    "            test_loss = criterion(tau_hat, test_y)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch} loss: {running_loss}, test MSE loss: {test_loss}\")\n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 13057.868942260742, test MSE loss: 0.4878769814968109\n",
      "Epoch 1 loss: 12794.155258178711, test MSE loss: 0.19426460564136505\n",
      "Epoch 2 loss: 12652.877616882324, test MSE loss: 0.11436960101127625\n",
      "Epoch 3 loss: 12581.183120727539, test MSE loss: 0.0980927124619484\n",
      "Epoch 4 loss: 12532.575618743896, test MSE loss: 0.14061124622821808\n",
      "Epoch 5 loss: 12500.087909698486, test MSE loss: 0.17480342090129852\n",
      "Epoch 6 loss: 12479.085945129395, test MSE loss: 0.16939277946949005\n",
      "Epoch 7 loss: 12455.146656036377, test MSE loss: 0.15010599792003632\n",
      "Epoch 8 loss: 12423.807285308838, test MSE loss: 0.14645074307918549\n",
      "Epoch 9 loss: 12385.12236404419, test MSE loss: 0.10044682770967484\n",
      "Epoch 10 loss: 12353.118217468262, test MSE loss: 0.10722638666629791\n",
      "Epoch 11 loss: 12329.308185577393, test MSE loss: 0.14364682137966156\n",
      "Epoch 12 loss: 12310.638423919678, test MSE loss: 0.08175883442163467\n",
      "Epoch 13 loss: 12285.488231658936, test MSE loss: 0.11123448610305786\n",
      "Epoch 14 loss: 12259.470500946045, test MSE loss: 0.1028904914855957\n",
      "Epoch 15 loss: 12236.447311401367, test MSE loss: 0.0807255432009697\n",
      "Epoch 16 loss: 12220.082885742188, test MSE loss: 0.08147502690553665\n",
      "Epoch 17 loss: 12207.688785552979, test MSE loss: 0.10269676893949509\n",
      "Epoch 18 loss: 12193.751670837402, test MSE loss: 0.121180459856987\n",
      "Epoch 19 loss: 12182.262310028076, test MSE loss: 0.08249304443597794\n",
      "Epoch 20 loss: 12161.527965545654, test MSE loss: 0.07249399274587631\n",
      "Epoch 21 loss: 12141.24447631836, test MSE loss: 0.07853548973798752\n",
      "Epoch 22 loss: 12127.917461395264, test MSE loss: 0.05990098789334297\n",
      "Epoch 23 loss: 12120.797603607178, test MSE loss: 0.08735375851392746\n",
      "Epoch 24 loss: 12114.11502456665, test MSE loss: 0.07295573502779007\n",
      "Epoch 25 loss: 12106.300079345703, test MSE loss: 0.07389873266220093\n",
      "Epoch 26 loss: 12096.89867401123, test MSE loss: 0.06403118371963501\n",
      "Epoch 27 loss: 12085.451099395752, test MSE loss: 0.08960379660129547\n",
      "Epoch 28 loss: 12072.901706695557, test MSE loss: 0.07559537142515182\n",
      "Epoch 29 loss: 12068.366512298584, test MSE loss: 0.07607968151569366\n",
      "Epoch 30 loss: 12056.31954574585, test MSE loss: 0.05835263431072235\n",
      "Epoch 31 loss: 12045.654155731201, test MSE loss: 0.0740007534623146\n",
      "Epoch 32 loss: 12042.150562286377, test MSE loss: 0.07642988860607147\n",
      "Epoch 33 loss: 12040.055141448975, test MSE loss: 0.09127043932676315\n",
      "Epoch 34 loss: 12040.580043792725, test MSE loss: 0.07201461493968964\n",
      "Epoch 35 loss: 12040.466079711914, test MSE loss: 0.06282315403223038\n",
      "Epoch 36 loss: 12040.052169799805, test MSE loss: 0.07784911245107651\n",
      "Epoch 37 loss: 12035.428932189941, test MSE loss: 0.06916315108537674\n",
      "Epoch 38 loss: 12025.815910339355, test MSE loss: 0.06044194847345352\n",
      "Epoch 39 loss: 12012.58720779419, test MSE loss: 0.060418255627155304\n",
      "Epoch 40 loss: 11997.018264770508, test MSE loss: 0.06250258535146713\n",
      "Epoch 41 loss: 11993.194980621338, test MSE loss: 0.06475882232189178\n",
      "Epoch 42 loss: 11995.447597503662, test MSE loss: 0.08468649536371231\n",
      "Epoch 43 loss: 11994.653453826904, test MSE loss: 0.08237577974796295\n",
      "Epoch 44 loss: 11993.242282867432, test MSE loss: 0.06406491249799728\n",
      "Epoch 45 loss: 11991.109169006348, test MSE loss: 0.07649452239274979\n",
      "Epoch 46 loss: 11987.715320587158, test MSE loss: 0.049621231853961945\n",
      "Epoch 47 loss: 11991.064266204834, test MSE loss: 0.047997042536735535\n",
      "Epoch 48 loss: 11980.550788879395, test MSE loss: 0.07641609758138657\n",
      "Epoch 49 loss: 11968.222743988037, test MSE loss: 0.0789022222161293\n",
      "Epoch 50 loss: 11954.815254211426, test MSE loss: 0.054994769394397736\n",
      "Epoch 51 loss: 11949.029441833496, test MSE loss: 0.05333154648542404\n",
      "Epoch 52 loss: 11947.034030914307, test MSE loss: 0.054181937128305435\n",
      "Epoch 53 loss: 11947.905132293701, test MSE loss: 0.05234494060277939\n",
      "Epoch 54 loss: 11950.842254638672, test MSE loss: 0.057289011776447296\n",
      "Epoch 55 loss: 11954.670677185059, test MSE loss: 0.05589693784713745\n",
      "Epoch 56 loss: 11957.006649017334, test MSE loss: 0.082867830991745\n",
      "Epoch 57 loss: 11957.11845779419, test MSE loss: 0.07982780784368515\n",
      "Epoch 58 loss: 11956.533470153809, test MSE loss: 0.04699629172682762\n",
      "Epoch 59 loss: 11954.259757995605, test MSE loss: 0.1071191281080246\n",
      "Epoch 60 loss: 11953.921600341797, test MSE loss: 0.07466597855091095\n",
      "Epoch 61 loss: 11953.368614196777, test MSE loss: 0.05261105298995972\n",
      "Epoch 62 loss: 11947.675117492676, test MSE loss: 0.05332980677485466\n",
      "Epoch 63 loss: 11932.913967132568, test MSE loss: 0.05359506234526634\n",
      "Epoch 64 loss: 11921.89566040039, test MSE loss: 0.05126504227519035\n",
      "Epoch 65 loss: 11919.999668121338, test MSE loss: 0.051166124641895294\n",
      "Epoch 66 loss: 11915.845756530762, test MSE loss: 0.07380173355340958\n",
      "Epoch 67 loss: 11913.408233642578, test MSE loss: 0.04850377142429352\n",
      "Epoch 68 loss: 11917.927341461182, test MSE loss: 0.0531618595123291\n",
      "Epoch 69 loss: 11920.365520477295, test MSE loss: 0.04759868234395981\n",
      "Epoch 70 loss: 11923.194915771484, test MSE loss: 0.0607917495071888\n",
      "Epoch 71 loss: 11925.624828338623, test MSE loss: 0.0749329999089241\n",
      "Epoch 72 loss: 11925.326950073242, test MSE loss: 0.06310614943504333\n",
      "Epoch 73 loss: 11923.991889953613, test MSE loss: 0.05130031704902649\n",
      "Epoch 74 loss: 11917.026401519775, test MSE loss: 0.0774911567568779\n",
      "Epoch 75 loss: 11926.949504852295, test MSE loss: 0.08105359226465225\n",
      "Epoch 76 loss: 11932.858963012695, test MSE loss: 0.07158492505550385\n",
      "Epoch 77 loss: 11933.82335281372, test MSE loss: 0.0688571184873581\n",
      "Epoch 78 loss: 11914.780403137207, test MSE loss: 0.04626736417412758\n",
      "Epoch 79 loss: 11900.49526977539, test MSE loss: 0.052616186439991\n",
      "Epoch 80 loss: 11896.660945892334, test MSE loss: 0.04696895182132721\n",
      "Epoch 81 loss: 11901.930332183838, test MSE loss: 0.0470799095928669\n",
      "Epoch 82 loss: 11906.327816009521, test MSE loss: 0.050807468593120575\n",
      "Epoch 83 loss: 11908.630352020264, test MSE loss: 0.04712787643074989\n",
      "Epoch 84 loss: 11903.822624206543, test MSE loss: 0.05382692441344261\n",
      "Epoch 85 loss: 11896.13465499878, test MSE loss: 0.05372067540884018\n",
      "Epoch 86 loss: 11899.199928283691, test MSE loss: 0.05821028724312782\n",
      "Epoch 87 loss: 11907.909214019775, test MSE loss: 0.0985373854637146\n",
      "Epoch 88 loss: 11917.197856903076, test MSE loss: 0.061254095286130905\n",
      "Epoch 89 loss: 11920.864730834961, test MSE loss: 0.05802514776587486\n",
      "Epoch 90 loss: 11908.143634796143, test MSE loss: 0.059031520038843155\n",
      "Epoch 91 loss: 11896.568424224854, test MSE loss: 0.04819277301430702\n",
      "Epoch 92 loss: 11883.379005432129, test MSE loss: 0.042450495064258575\n",
      "Epoch 93 loss: 11880.155601501465, test MSE loss: 0.05257648602128029\n",
      "Epoch 94 loss: 11888.733131408691, test MSE loss: 0.05725311487913132\n",
      "Epoch 95 loss: 11889.5791015625, test MSE loss: 0.03947918862104416\n",
      "Epoch 96 loss: 11887.52205657959, test MSE loss: 0.03611345216631889\n",
      "Epoch 97 loss: 11885.053165435791, test MSE loss: 0.03614594042301178\n",
      "Epoch 98 loss: 11880.291080474854, test MSE loss: 0.056799136102199554\n",
      "Epoch 99 loss: 11876.79578781128, test MSE loss: 0.05459928885102272\n"
     ]
    }
   ],
   "source": [
    "# initialize the joint autoencoder\n",
    "input_dim = 3 * n_samples\n",
    "latent_dim = 20\n",
    "hidden_dim = 128\n",
    "num_epochs = 1\n",
    "lr = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "for pi in datasets.keys():\n",
    "    print(pi)\n",
    "    train_data = datasets[pi]['train_data']\n",
    "    test_data = datasets[pi]['test_data']\n",
    "    model = JointAutoencoder(input_dim, latent_dim, hidden_dim)\n",
    "    model = train_joint_autoencoder(\n",
    "        model,\n",
    "        train_data,\n",
    "        test_data,\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        device,\n",
    "        verbose=True,\n",
    "    )\n",
    "    model.save(f\"joint_autoencoder_{pi}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
