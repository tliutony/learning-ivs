{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Notebook for learning linear IVs with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from ipywidgets import interact_manual, IntSlider, FloatSlider\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from linearmodels.iv.model import IV2SLS\n",
    "from linearmodels.iv.model import _OLS\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_const_linear_iv(\n",
    "    n_samples,\n",
    "    seed,\n",
    "    pi,\n",
    "    psi,\n",
    "    tau,\n",
    "    gamma\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates linear IV with constant treatment effects.\n",
    "    \n",
    "    Args:\n",
    "        n_samples (int): num samples to generate\n",
    "        seed (int): seed for reproducibilty\n",
    "        pi (float): instrument \"strength\"\n",
    "        psi (float): confounding \"strength\"\n",
    "        tau (float): treatment effect\n",
    "        gamma (float): confound effect\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    np.random.seed(seed),\n",
    "    Z = np.random.normal(0, 1, size=n_samples)#np.random.uniform(0, 10, n_samples)\n",
    "    C = np.random.normal(0, 1, size=n_samples)#np.random.uniform(0, 10, n_samples)\n",
    "    eta = np.random.normal(0, 1, size=n_samples)\n",
    "    const = np.random.uniform(-1, 1)\n",
    "\n",
    "    T = const + (pi*Z) + (psi*C) + eta\n",
    "\n",
    "    epsilon = np.random.normal(0, 1, size=n_samples)\n",
    "    beta = np.random.uniform(-1, 1)\n",
    "\n",
    "    Y = beta + (tau*T) + (gamma*C) + epsilon\n",
    "\n",
    "    data = np.concatenate([Z.reshape(-1,1), \n",
    "                           C.reshape(-1,1), \n",
    "                           T.reshape(-1,1),\n",
    "                           Y.reshape(-1,1),], \n",
    "                         axis=1)\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=['Z', 'C', 'T', 'Y'])\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:34<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "\n",
    "}\n",
    "\n",
    "n_datasets = 10000\n",
    "n_test_datasets = 2000\n",
    "iv_strs = np.round(np.linspace(0, 2, 11), 2) \n",
    "\n",
    "for pi in tqdm(iv_strs):\n",
    "    datasets[pi] = {\n",
    "        \"data\": [],\n",
    "        \"taus\": np.zeros(n_datasets),\n",
    "        \"confounds\": np.zeros(n_datasets),\n",
    "        \"data_tup\": [],\n",
    "    }\n",
    "    # data = []#np.zeros((n_datasets, 10))\n",
    "    # taus = np.zeros(n_datasets)\n",
    "    # confounds = np.zeros(n_datasets)\n",
    "    \n",
    "    for i in range(n_datasets):\n",
    "        seed = i + int(pi*10000) # to ensure we have non-overlapping datasets\n",
    "        np.random.seed(seed)\n",
    "        treat_effect = np.random.uniform(-2, 2)\n",
    "        confound_effect = 5 #np.random.uniform(1, 5) # hold confounding constant, for now TODO\n",
    "        psi_effect = np.random.uniform(5, 10)\n",
    "   \n",
    "        n_samples = 1000\n",
    "        \n",
    "        data_df = generate_const_linear_iv(\n",
    "            n_samples=n_samples,\n",
    "            seed=seed,\n",
    "            pi=pi,\n",
    "            psi=psi_effect,\n",
    "            tau=treat_effect,\n",
    "            gamma=confound_effect)\n",
    "        \n",
    "        # feats = generate_iv_features(data_df)\n",
    "        # data[i,:] = feats\n",
    "\n",
    "        # # zero out the variance of Y and variance of T\n",
    "        # data[i, feat_cols.index(\"var_Y\")] = 0\n",
    "        # data[i, feat_cols.index(\"var_T\")] = 0\n",
    "\n",
    "        datasets[pi]['data'].append(data_df)\n",
    "        datasets[pi]['taus'][i] = treat_effect\n",
    "        datasets[pi]['confounds'][i] = confound_effect\n",
    "\n",
    "        # convert data_df and tau to torch dataloader\n",
    "        datasets[pi]['data_tup'].append((data_df.drop(\"C\", axis='columns').values.astype('float32'), treat_effect))\n",
    "\n",
    "        # convert datasets data and taus to torch dataloader\n",
    "        train_data = torch.utils.data.DataLoader(\n",
    "            datasets[pi]['data_tup'][:n_datasets - n_test_datasets],\n",
    "            batch_size=32,\n",
    "        )\n",
    "\n",
    "        test_data = torch.utils.data.DataLoader(\n",
    "            datasets[pi]['data_tup'][n_test_datasets:],\n",
    "            batch_size=n_test_datasets,\n",
    "        )\n",
    "\n",
    "        datasets[pi]['train_data'] = train_data\n",
    "        datasets[pi]['test_data'] = test_data\n",
    "\n",
    "# for pi in datasets.keys():\n",
    "#     datasets[pi]['data'] = torch.utils.data.DataLoader(\n",
    "#         datasets[pi]['data'],\n",
    "#         batch_size=32,\n",
    "#         shuffle=True,\n",
    "#         num_workers=0,\n",
    "#     )\n",
    "#     datasets[pi]['taus'] = torch.from_numpy(datasets[pi]['taus']).float()\n",
    "#     datasets[pi]['confounds'] = torch.from_numpy(datasets[pi]['confounds']).float()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(datasets, open(\"c9/datasets.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint autoencoder + treatment effect model\n",
    "class JointAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim):\n",
    "        super(JointAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "\n",
    "        self.treatment = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        z = self.encoder(x)\n",
    "        tau_hat = self.treatment(z)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z, tau_hat\n",
    "    \n",
    "\n",
    "# train the joint autoencoder\n",
    "def train_joint_autoencoder(\n",
    "    model,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    num_epochs,\n",
    "    lr,\n",
    "    device,\n",
    "    verbose=False,\n",
    "    batch_size=32,\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    test_x, test_y = next(iter(test_data))\n",
    "    test_x = test_x.to(torch.float32).to(device)\n",
    "    test_y = test_y.to(torch.float32).to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (data, labels) in enumerate(train_data):\n",
    "            data = data.to(torch.float32).to(device)\n",
    "            labels = labels.to(torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, z, tau_hat = model(data)\n",
    "            x_hat = x_hat.view(batch_size, -1, 3)\n",
    "            loss = criterion(x_hat, data) + criterion(tau_hat.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, _, tau_hat = model(test_x)\n",
    "            tau_hat = tau_hat.squeeze()\n",
    "            test_loss = criterion(tau_hat, test_y)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch} loss: {running_loss}, test MSE loss: {test_loss}\")\n",
    "            \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Epoch 0 loss: 14147.074440002441, test MSE loss: 3.0514118671417236\n",
      "0.2\n",
      "Epoch 0 loss: 14206.853603363037, test MSE loss: 2.3747122287750244\n",
      "0.4\n",
      "Epoch 0 loss: 14205.530284881592, test MSE loss: 1.1763437986373901\n",
      "0.6\n",
      "Epoch 0 loss: 14167.255794525146, test MSE loss: 1.4496040344238281\n",
      "0.8\n",
      "Epoch 0 loss: 14129.18367767334, test MSE loss: 2.09649658203125\n",
      "1.0\n",
      "Epoch 0 loss: 14327.093772888184, test MSE loss: 1.2923985719680786\n",
      "1.2\n",
      "Epoch 0 loss: 14449.903553009033, test MSE loss: 5.58336877822876\n",
      "1.4\n",
      "Epoch 0 loss: 14667.4818649292, test MSE loss: 3.3748669624328613\n",
      "1.6\n",
      "Epoch 0 loss: 14796.764713287354, test MSE loss: 3.143280506134033\n",
      "1.8\n",
      "Epoch 0 loss: 14800.747718811035, test MSE loss: 1.2312290668487549\n",
      "2.0\n",
      "Epoch 0 loss: 15007.906093597412, test MSE loss: 3.786665678024292\n"
     ]
    }
   ],
   "source": [
    "# initialize the joint autoencoder\n",
    "input_dim = 3 * n_samples\n",
    "latent_dim = 20\n",
    "hidden_dim = 128\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "for pi in datasets.keys():\n",
    "    print(pi)\n",
    "    train_data = datasets[pi]['train_data']\n",
    "    test_data = datasets[pi]['test_data']\n",
    "    model = JointAutoencoder(input_dim, latent_dim, hidden_dim)\n",
    "    model = train_joint_autoencoder(\n",
    "        model,\n",
    "        train_data,\n",
    "        test_data,\n",
    "        num_epochs,\n",
    "        lr,\n",
    "        device,\n",
    "        verbose=True,\n",
    "    )\n",
    "    torch.save(model, f\"joint_autoencoder_{pi}.pt\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
